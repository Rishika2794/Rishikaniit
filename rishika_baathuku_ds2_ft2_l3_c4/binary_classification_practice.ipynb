{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9cb062",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90d6863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import PIL, PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0636d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, tensorflow.keras.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfad67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d29d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the dataset\n",
    "data_dir = utils.get_file(origin=url, fname=\"flower_photos\",untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "670481e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anant\\.keras\\datasets\\flower_photos\n"
     ]
    }
   ],
   "source": [
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9abddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view imgs stored in the directory\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd813eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2641b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3670"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(data_dir.glob('*/*.jpg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa80c86",
   "metadata": {},
   "source": [
    "# Binary classification using tensor flow\n",
    "## classes: cats, dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c9d6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam  #controls lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e678243",
   "metadata": {},
   "source": [
    "# create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa75d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialise the bit depth\n",
    "# for color imgs it is 3 for monochrome it is 1\n",
    "bitdepth = 3\n",
    "width = 64\n",
    "height = 64\n",
    "nodes = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81f5ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialise the model\n",
    "classifier = Sequential()\n",
    "\n",
    "# design network in the following sequence\n",
    "# Convolutional layer, pooling layer, flatten layer, fully connected layer\n",
    "'''\n",
    "p1: total no of filters\n",
    "p2:kernel size\n",
    "p3:image shape[width, height, bitdepth]\n",
    "\n",
    "'''\n",
    "#convolution layer\n",
    "classifier.add(Convolution2D(64,kernel_size=(2,2),input_shape=(width,height,bitdepth),activation='relu'))\n",
    "classifier.add(Conv2D(32,kernel_size=(2,2),activation='relu'))\n",
    "\n",
    "# max pooling layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#flatten layer\n",
    "classifier.add(Flatten())\n",
    "\n",
    "#fully connected layer \n",
    "classifier.add(Dense(units=nodes,activation='relu')) #HL1\n",
    "classifier.add(Dropout(rate=0.2)) #dropout layer1\n",
    "\n",
    "classifier.add(Dense(units=nodes-5,activation='relu')) #HL2\n",
    "classifier.add(Dropout(rate=0.2)) #dropout layer2\n",
    "\n",
    "classifier.add(Dense(units=1,activation='sigmoid')) #output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49e9d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "classifier.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f4ad13",
   "metadata": {},
   "source": [
    "# Image processing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9efd718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69f85bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anant\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:337: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "# apply image augmentation on train data\n",
    "train_datagen = ImageDataGenerator(rescale=1/255, zoom_range=0.25, shear_range=0.1, zca_whitening=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dad9e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply image augmentation on validation data\n",
    "val_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4754ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the training and validation directories to read the image files\n",
    "train_dir =r\"C:\\Users\\anant\\Downloads\\data_flowers\\data_flowers\\trainfl\"\n",
    "val_dir=r\"C:\\Users\\anant\\Downloads\\data_flowers\\data_flowers\\valflo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1de05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=1\n",
    "classmode='binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f554ad02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1034 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# read the files(train)\n",
    "train_set = train_datagen.flow_from_directory(train_dir,target_size=(width,height),batch_size=batchsize,\n",
    "                                             class_mode=classmode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6818c820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# read the files(validation)\n",
    "val_set = val_datagen.flow_from_directory(val_dir,target_size=(width,height),batch_size=batchsize,\n",
    "                                             class_mode=classmode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dd6fac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count = 1034, Validation count = 200\n"
     ]
    }
   ],
   "source": [
    "# count the images from training and validations\n",
    "train_count = train_set.n\n",
    "val_count = val_set.n\n",
    "print('Train count = {}, Validation count = {}'.format(train_count,val_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1c020",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8ce1aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f5657d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anant\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\anant\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:739: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.4910 - accuracy: 0.7872 - val_loss: 0.3055 - val_accuracy: 0.8500\n",
      "Epoch 2/30\n",
      "1034/1034 [==============================] - 8s 8ms/step - loss: 0.3661 - accuracy: 0.8520 - val_loss: 0.3286 - val_accuracy: 0.8550\n",
      "Epoch 3/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.3230 - accuracy: 0.8733 - val_loss: 0.2910 - val_accuracy: 0.8850\n",
      "Epoch 4/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.2966 - accuracy: 0.8859 - val_loss: 0.3562 - val_accuracy: 0.8300\n",
      "Epoch 5/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.2554 - accuracy: 0.8975 - val_loss: 0.7959 - val_accuracy: 0.8200\n",
      "Epoch 6/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.3037 - accuracy: 0.8830 - val_loss: 0.2455 - val_accuracy: 0.9000\n",
      "Epoch 7/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.2110 - accuracy: 0.9207 - val_loss: 0.3207 - val_accuracy: 0.8850\n",
      "Epoch 8/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.2418 - accuracy: 0.9130 - val_loss: 0.3743 - val_accuracy: 0.8650\n",
      "Epoch 9/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.1940 - accuracy: 0.9236 - val_loss: 0.2117 - val_accuracy: 0.9250\n",
      "Epoch 10/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.1835 - accuracy: 0.9323 - val_loss: 0.2248 - val_accuracy: 0.9150\n",
      "Epoch 11/30\n",
      "1034/1034 [==============================] - 9s 9ms/step - loss: 0.2201 - accuracy: 0.9381 - val_loss: 0.3488 - val_accuracy: 0.8550\n",
      "Epoch 12/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.1906 - accuracy: 0.9342 - val_loss: 0.1950 - val_accuracy: 0.9100\n",
      "Epoch 13/30\n",
      "1034/1034 [==============================] - 9s 9ms/step - loss: 0.1511 - accuracy: 0.9429 - val_loss: 0.3122 - val_accuracy: 0.9150\n",
      "Epoch 14/30\n",
      "1034/1034 [==============================] - 9s 9ms/step - loss: 0.1582 - accuracy: 0.9487 - val_loss: 0.2040 - val_accuracy: 0.9300\n",
      "Epoch 15/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.1053 - accuracy: 0.9603 - val_loss: 0.3969 - val_accuracy: 0.8850\n",
      "Epoch 16/30\n",
      "1034/1034 [==============================] - 9s 9ms/step - loss: 0.1254 - accuracy: 0.9565 - val_loss: 0.1849 - val_accuracy: 0.9400\n",
      "Epoch 17/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.0939 - accuracy: 0.9662 - val_loss: 0.4125 - val_accuracy: 0.9300\n",
      "Epoch 18/30\n",
      "1034/1034 [==============================] - 9s 9ms/step - loss: 0.1313 - accuracy: 0.9507 - val_loss: 0.5080 - val_accuracy: 0.8900\n",
      "Epoch 19/30\n",
      "1034/1034 [==============================] - 9s 9ms/step - loss: 0.0975 - accuracy: 0.9632 - val_loss: 0.2561 - val_accuracy: 0.9400\n",
      "Epoch 20/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.1262 - accuracy: 0.9623 - val_loss: 0.3009 - val_accuracy: 0.9150\n",
      "Epoch 21/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.1106 - accuracy: 0.9603 - val_loss: 0.2958 - val_accuracy: 0.9050\n",
      "Epoch 22/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.0851 - accuracy: 0.9729 - val_loss: 0.3500 - val_accuracy: 0.9200\n",
      "Epoch 23/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.0843 - accuracy: 0.9729 - val_loss: 0.2649 - val_accuracy: 0.9400\n",
      "Epoch 24/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.0733 - accuracy: 0.9758 - val_loss: 0.3442 - val_accuracy: 0.9500\n",
      "Epoch 25/30\n",
      "1034/1034 [==============================] - 9s 9ms/step - loss: 0.0833 - accuracy: 0.9758 - val_loss: 0.3429 - val_accuracy: 0.9350\n",
      "Epoch 26/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.0783 - accuracy: 0.9720 - val_loss: 0.5051 - val_accuracy: 0.9100\n",
      "Epoch 27/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.0595 - accuracy: 0.9807 - val_loss: 0.4875 - val_accuracy: 0.9050\n",
      "Epoch 28/30\n",
      "1034/1034 [==============================] - 9s 9ms/step - loss: 0.1348 - accuracy: 0.9642 - val_loss: 0.3197 - val_accuracy: 0.9150\n",
      "Epoch 29/30\n",
      "1034/1034 [==============================] - 9s 9ms/step - loss: 0.0699 - accuracy: 0.9739 - val_loss: 0.6533 - val_accuracy: 0.8950\n",
      "Epoch 30/30\n",
      "1034/1034 [==============================] - 9s 8ms/step - loss: 0.0762 - accuracy: 0.9700 - val_loss: 0.5038 - val_accuracy: 0.9100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e381a57310>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_set,steps_per_epoch=train_count,epochs=epoch,\n",
    "              validation_data = val_set,validation_steps = val_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4eaab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the images from the test data\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d7469e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES=['daisy','sunflowers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e995173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the test path\n",
    "test_path=r\"C:\\Users\\anant\\Downloads\\data_flowers\\data_flowers\\testflo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43837b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list is to store the filenames of the test data\n",
    "test_images=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e91202ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\anant\\\\Downloads\\\\data_flowers\\\\data_flowers\\\\testflo/16988605969_570329ff20_n.jpg', 'C:\\\\Users\\\\anant\\\\Downloads\\\\data_flowers\\\\data_flowers\\\\testflo/18097401209_910a46fae1_n.jpg', 'C:\\\\Users\\\\anant\\\\Downloads\\\\data_flowers\\\\data_flowers\\\\testflo/20329326505_a777c71cc2.jpg', 'C:\\\\Users\\\\anant\\\\Downloads\\\\data_flowers\\\\data_flowers\\\\testflo/20685027271_0e7306e7c1_n.jpg']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _,_,files in os.walk(test_path):\n",
    "    for f in files:\n",
    "        test_images.append(test_path + f)\n",
    "print(test_images)\n",
    "len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ee6de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the images in numpy array format\n",
    "image_stack= None\n",
    "for i in test_images:\n",
    "    img=image.load_img(i,target_size=(64,64))\n",
    "    \n",
    "    #converting the individual image into array\n",
    "    y=image.img_to_array(img)\n",
    "    y=np.expand_dims(y,axis=0)\n",
    "    \n",
    "    #convert the image as in scale data\n",
    "    y/=255\n",
    "    \n",
    "    #stack every transformed image\n",
    "    if image_stack is None:\n",
    "        image_stack=y\n",
    "    else:\n",
    "        image_stack=np.vstack([image_stack,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57413f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e160ec8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.99607843, 0.99607843, 0.99607843],\n",
       "        [0.6431373 , 0.9764706 , 0.99607843],\n",
       "        [0.6039216 , 0.8980392 , 1.        ],\n",
       "        ...,\n",
       "        [0.87058824, 0.9607843 , 0.99215686],\n",
       "        [0.8235294 , 0.92941177, 1.        ],\n",
       "        [0.78431374, 0.89411765, 0.98039216]],\n",
       "\n",
       "       [[0.99215686, 0.9843137 , 0.99607843],\n",
       "        [0.972549  , 0.99215686, 1.        ],\n",
       "        [0.7607843 , 0.96862745, 0.99215686],\n",
       "        ...,\n",
       "        [0.76862746, 0.89411765, 0.99215686],\n",
       "        [0.7411765 , 0.8666667 , 0.9647059 ],\n",
       "        [0.6901961 , 0.827451  , 0.9372549 ]],\n",
       "\n",
       "       [[0.89411765, 0.99607843, 0.9843137 ],\n",
       "        [0.9843137 , 0.9843137 , 0.9764706 ],\n",
       "        [0.64705884, 0.9529412 , 0.99215686],\n",
       "        ...,\n",
       "        [0.6901961 , 0.81960785, 0.9411765 ],\n",
       "        [0.6784314 , 0.80784315, 0.9372549 ],\n",
       "        [0.6627451 , 0.7921569 , 0.92156863]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.2901961 , 0.38039216, 0.26666668],\n",
       "        [0.46666667, 0.5529412 , 0.29803923],\n",
       "        [0.4509804 , 0.5764706 , 0.3137255 ],\n",
       "        ...,\n",
       "        [0.36862746, 0.29803923, 0.20392157],\n",
       "        [0.14117648, 0.11372549, 0.01176471],\n",
       "        [0.12156863, 0.08627451, 0.02745098]],\n",
       "\n",
       "       [[0.2       , 0.3019608 , 0.16470589],\n",
       "        [0.10196079, 0.21960784, 0.07058824],\n",
       "        [0.43137255, 0.5137255 , 0.36078432],\n",
       "        ...,\n",
       "        [0.16470589, 0.11372549, 0.05098039],\n",
       "        [0.2       , 0.09019608, 0.00784314],\n",
       "        [0.13333334, 0.0627451 , 0.05490196]],\n",
       "\n",
       "       [[0.01960784, 0.10980392, 0.00784314],\n",
       "        [0.13333334, 0.23921569, 0.07843138],\n",
       "        [0.07058824, 0.1764706 , 0.04705882],\n",
       "        ...,\n",
       "        [0.21568628, 0.2       , 0.0627451 ],\n",
       "        [0.28235295, 0.15686275, 0.10588235],\n",
       "        [0.16078432, 0.08235294, 0.08627451]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1668c90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(classifier.predict(image_stack),-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "017a0e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['daisy', 'daisy', 'daisy', 'daisy']\n"
     ]
    }
   ],
   "source": [
    "# store the actual classes\n",
    "preds = [(lambda c: 'daisy' if c==0 else 'sunflowers') (c) for c in predictions]\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d1d73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual=['sunflowers','sunflowers','daisy','daisy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "158aaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame({'actual':actual,'predicted':preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bcdcdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted</th>\n",
       "      <th>daisy</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>daisy</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunflowers</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted   daisy  All\n",
       "actual                \n",
       "daisy           2    2\n",
       "sunflowers      2    2\n",
       "All             4    4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df1.actual,df1.predicted,margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c36ccec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "print('Accuracy = {}'.format(accuracy_score(df1.actual,df1.predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f89a229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       daisy       0.50      1.00      0.67         2\n",
      "  sunflowers       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.50         4\n",
      "   macro avg       0.25      0.50      0.33         4\n",
      "weighted avg       0.25      0.50      0.33         4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anant\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\anant\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\anant\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "print(classification_report(df1.actual,df1.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc264b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
